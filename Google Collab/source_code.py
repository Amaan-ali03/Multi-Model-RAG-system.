# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FHrlkrdSObLxx7hprusQPXvfJ7sXqxWG
"""

pip install faiss-cpu

import pandas as pd
import torch
import torch.nn as nn
import numpy as np
import faiss  # Vector similarity search
from torch.utils.data import Dataset, DataLoader
from transformers import (
    BertTokenizer,
    BertModel,
    AutoTokenizer,
    AutoModelForSequenceClassification
)

df = pd.read_csv("/content/Aspect_Complain_web_entity_clip_new1-2.csv")

text_column = "Complaint/ Opinion"

def preprocess_text(text):
    if isinstance(text, str):
        return text.lower().strip()
    return ""

df[text_column] = df[text_column].apply(preprocess_text)

class ClassificationDataset(Dataset):
    def __init__(self, df, text_col, label_col, label_map):
        self.texts = df[text_col].tolist()
        # Converting labels using the provided label_map
        self.labels = [label_map[label] for label in df[label_col]]
        self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            max_length=128,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "labels": torch.tensor(self.labels[idx], dtype=torch.long)
        }

# 3. General Training Function

def train_model(df, text_col, label_col, model_save_path):
    # Label map
    unique_labels = df[label_col].unique()
    label_map = {label: i for i, label in enumerate(unique_labels)}

    # Creating dataset and dataloader
    dataset = ClassificationDataset(df, text_col, label_col, label_map)
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

    # model for classification
    model = AutoModelForSequenceClassification.from_pretrained(
        "bert-base-uncased",
        num_labels=len(label_map)
    )
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
    epochs = 2
    model.train()

    for epoch in range(epochs):
        for batch in dataloader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

    model.save_pretrained(model_save_path)
    tokenizer.save_pretrained(model_save_path)

# 4. Training Individual Models for Each Task

aspect_col          = "Aspects_1"
cause_col           = "Cause_1"
complaint_cause_col = "Complaint_Cause"
severity_col        = "Severity level"
sentiment_col       = "Sentiment"
emotion_col         = "Emotion"

# Example of training each model.

train_model(df, text_column, aspect_col,          "aspect_model")
train_model(df, text_column, cause_col,           "cause_model")
train_model(df, text_column, complaint_cause_col, "complaint_cause_model")
train_model(df, text_column, severity_col,        "severity_model")
train_model(df, text_column, sentiment_col,       "sentiment_model")
train_model(df, text_column, emotion_col,         "emotion_model")

# 5. Building Embeddings for Retrieval with BERT + FAISS

import os
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
import os
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'nouvm'
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"
torch.cuda.empty_cache()


bert_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert_for_embeddings = BertModel.from_pretrained("bert-base-uncased")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bert_for_embeddings.to(device)
bert_for_embeddings.eval()

def get_embeddings(text_list):
    inputs = bert_tokenizer(
        text_list,
        max_length=128,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = bert_for_embeddings(**inputs)
    # Mean pooling
    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()

embeddings = get_embeddings(df[text_column].tolist())

dim = embeddings.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(embeddings)

# 6. Example: Loading One Trained Model (Sentiment)
saved_sentiment_model = AutoModelForSequenceClassification.from_pretrained("sentiment_model")
saved_sentiment_model.to(device)
saved_sentiment_model.eval()

# Suppose we have a map for sentiment predictions
sentiment_map = {}
for i, lbl in enumerate(df[sentiment_col].unique()):
    sentiment_map[i] = lbl

# 7. Retrieval-Based Classification Example

def retrieve_similar_examples(query_text, k=3):
    query_embedding = get_embeddings([query_text])
    _, idxs = index.search(query_embedding, k)
    return df.iloc[idxs[0]]

def classify_review(review_text):
    # Retrieval
    retrieved_examples = retrieve_similar_examples(review_text, k=3)

    # A. Aspect
    top_aspect = retrieved_examples[aspect_col].value_counts().idxmax()

    # B. Cause
    top_cause = retrieved_examples[cause_col].value_counts().idxmax()

    # C. Complaint Cause
    top_complaint_cause = retrieved_examples[complaint_cause_col].value_counts().idxmax()

    # D. Severity
    top_severity = retrieved_examples[severity_col].value_counts().idxmax()

    # E. Sentiment (from loaded model)
    tokens = bert_tokenizer(
        review_text,
        max_length=128,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    )
    tokens = {k: v.to(device) for k, v in tokens.items()}
    with torch.no_grad():
        output = saved_sentiment_model(**tokens)
    pred_label = torch.argmax(output.logits, dim=1).item()
    predicted_sentiment = sentiment_map[pred_label]

    # F. Emotion
    top_emotion = retrieved_examples[emotion_col].value_counts().idxmax()

    return {
        "aspect": top_aspect,
        "cause": top_cause,
        "complaint_cause": top_complaint_cause,
        "severity": top_severity,
        "sentiment": predicted_sentiment,
        "emotion": top_emotion
    }

# 8. Example Usage
# --------------------------------------------------
example_review = (
    "@ICICIBank @ICICIBank_Care Stop cheating ICICI HPCL Coral credit card "
    "customers by not providing the entitled cashback even when the transaction "
    "is done at HPCL outlets. Your customer care doesn't actually care even "
    "with proper proof. Worse experience ever."
)
prediction = classify_review(example_review)
print("Sample Prediction:", prediction)